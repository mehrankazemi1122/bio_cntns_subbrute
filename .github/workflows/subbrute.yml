name: Continuous Subdomain Brute Force
 
on:
  push:
    branches: [ main ]
  schedule:
    # Runs every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch:

env:
  # CHANGE THIS TO YOUR TARGET DOMAIN
  TARGET_DOMAIN: "verisign.com" 
  DISCORD_TOKEN: ${{ secrets.DISCORD_TOKEN }}

jobs:
  # ==============================================================================
  # JOB 1: PREPARE
  # ==============================================================================
  prepare:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Install MassDNS & Utils
        run: |
          sudo apt-get install -y jq curl wget libpcap-dev crunch
          git clone https://github.com/blechschmidt/massdns.git
          cd massdns && make && sudo cp bin/massdns /usr/local/bin/

      - name: Generate Resolvers
        run: |
          wget https://raw.githubusercontent.com/trickest/resolvers/main/resolvers.txt -O raw_resolvers.txt
          echo 8.8.8.8 > trusted_resolver.txt
          massdns -r trusted_resolver.txt -t A -o J -w verified_resolvers.json --verify-ip raw_resolvers.txt -s 5000
          cat verified_resolvers.json | jq -r '.["name"]' | sed 's/\.$//' > resolvers.txt

      - name: Generate & Merge Wordlists
        run: |
          crunch 1 4 abcdefghijklmnopqrstuvwxyz1234567890 > 4_chars.txt
          
          curl -s https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/DNS/combined_subdomains.txt > combined.txt
          curl -s https://raw.githubusercontent.com/n0kovo/n0kovo_subdomains/main/n0kovo_subdomains_huge.txt > kovo.txt
          curl -s https://wordlists-cdn.assetnote.io/data/manual/best-dns-wordlist.txt > best-dns.txt
          curl -s https://wordlists-cdn.assetnote.io/data/manual/2m-subdomains.txt > 2m.txt
          curl -s https://wordlists-cdn.assetnote.io/data/automated/httparchive_subdomains_2024_01_28.txt > all_subs.txt
          curl -s https://media.githubusercontent.com/media/c3l3si4n/subdomain_data/refs/heads/main/common_nodes_6_occurrences.txt > 7mil.txt
          curl -s https://raw.githubusercontent.com/kimbieformist/bio_cntns_subbrute_v3ris1gn/refs/heads/main/spec_subs.txt > spec_subs.txt          
          
          cat best-dns.txt 2m.txt 4_chars.txt all_subs.txt combined.txt kovo.txt 7mil.txt spec_subs.txt | sort -u > merged_wordlist.txt
          echo "Total words: $(wc -l < merged_wordlist.txt)"

      - name: Split Wordlist
        run: |
          mkdir chunks
          # -a 2 ensures suffixes are 2 digits (00, 01... 19)
          split -d -a 2 -n l/20 merged_wordlist.txt chunks/wordlist_part_

      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: preparation-data
          # Upload the directory to preserve structure
          path: |
            resolvers.txt
            chunks

  # ==============================================================================
  # JOB 2: MATRIX SCAN
  # ==============================================================================
  scan:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        # QUOTED STRINGS to prevent YAML integer conversion issues
        part: ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']
    steps:
      - name: Download Preparation Data
        uses: actions/download-artifact@v4
        with:
          name: preparation-data
          path: .

      - name: Install Tools & Configure Path
        run: |
          sudo apt-get install -y libpcap-dev
          git clone https://github.com/blechschmidt/massdns.git
          cd massdns && make && sudo cp bin/massdns /usr/local/bin/
          cd ..
          
          go install -v github.com/projectdiscovery/shuffledns/cmd/shuffledns@latest
          go install -v github.com/projectdiscovery/dnsx/cmd/dnsx@latest
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest

          # Fix PATH
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH

      - name: Run Pipeline on Chunk ${{ matrix.part }}
        run: |
          WORDLIST="chunks/wordlist_part_${{ matrix.part }}"
          
          # 1. Shuffledns
          shuffledns -w $WORDLIST -d ${{ env.TARGET_DOMAIN }} -r resolvers.txt -mode bruteforce -retries 3 -o raw_subs_${{ matrix.part }}.txt
          
          # 2. Dnsx (Validate)
          if [ -s raw_subs_${{ matrix.part }}.txt ]; then
             cat raw_subs_${{ matrix.part }}.txt | grep "${{ env.TARGET_DOMAIN }}$" | dnsx -silent -r resolvers.txt -o valid_subs_${{ matrix.part }}.txt || true
          else
             touch valid_subs_${{ matrix.part }}.txt
          fi
          
          # 3. Httpx (Probe)
          if [ -s valid_subs_${{ matrix.part }}.txt ]; then
             httpx -l valid_subs_${{ matrix.part }}.txt \
               -follow-host-redirects -nc -title -status-code -cdn -tech-detect \
               -H "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:108.0) Gecko/20100101 Firefox/108.0" \
               -silent -o httpx_res_${{ matrix.part }}.txt || true
          else
             touch httpx_res_${{ matrix.part }}.txt
          fi

      - name: Upload Chunk Results
        uses: actions/upload-artifact@v4
        with:
          name: scan-results-${{ matrix.part }}
          path: |
            valid_subs_${{ matrix.part }}.txt
            httpx_res_${{ matrix.part }}.txt

  # ==============================================================================
  # JOB 3: FINALIZE
  # ==============================================================================
  finalize:
    needs: scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2

      - name: Download All Scan Results
        uses: actions/download-artifact@v4
        with:
          pattern: scan-results-*
          merge-multiple: true

      - name: Merge Results
        run: |
          cat valid_subs_*.txt | sort -u > current_subdomains.txt
          cat httpx_res_*.txt | sort -u > current_httpx.txt

      - name: Compare & Notify (HTTPX Only)
        run: |
          PREV_HTTPX="prev_httpx_res.txt"
          CURR_HTTPX="current_httpx.txt"
          touch $PREV_HTTPX
          
          # --- 1. Detect New Services ---
          awk '{print $1}' $PREV_HTTPX | sort > prev_urls.txt
          awk '{print $1}' $CURR_HTTPX | sort > curr_urls.txt
          
          comm -13 prev_urls.txt curr_urls.txt > new_urls_keys.txt
          
          if [ -s new_urls_keys.txt ]; then
            echo "New services found..."
            # grep finds lines, sed G adds an empty line after each result
            grep -Ff new_urls_keys.txt $CURR_HTTPX | sed G > added_services.txt
            
            MSG="**Httpx - Services Added:**\n\`\`\`\n$(cat added_services.txt | head -c 1900)\n\`\`\`"
            JSON_PAYLOAD=$(jq -n --arg content "$MSG" '{content: $content}')
            curl -H "Content-Type: application/json" -d "$JSON_PAYLOAD" "${{ env.DISCORD_TOKEN }}"
          fi
          
          # --- 2. Detect Status Changes ---
          python3 -c "
          import sys
          def load_map(fname):
              d = {}
              try:
                  with open(fname, 'r') as f:
                      for line in f:
                          parts = line.strip().split()
                          if len(parts) > 1:
                              d[parts[0]] = parts[1]
              except FileNotFoundError:
                  pass
              return d

          prev = load_map('$PREV_HTTPX')
          curr_lines = open('$CURR_HTTPX', 'r').readlines()
          
          changes = []
          for line in curr_lines:
              parts = line.strip().split()
              if len(parts) > 1:
                  url = parts[0]
                  status = parts[1]
                  if url in prev and prev[url] != status:
                      changes.append(f'URL: {url} | Old: {prev[url]} -> New: {status}')
          
          if changes:
              # CHANGED: Join with double newline for better Discord spacing
              print('\n\n'.join(changes))
          " > changed_services.txt
          
          if [ -s changed_services.txt ]; then
             echo "Status changes found..."
             MSG="**Httpx - Services Changed:**\n\`\`\`\n$(cat changed_services.txt | head -c 1900)\n\`\`\`"
             JSON_PAYLOAD=$(jq -n --arg content "$MSG" '{content: $content}')
             curl -H "Content-Type: application/json" -d "$JSON_PAYLOAD" "${{ env.DISCORD_TOKEN }}"
          fi
          
          # Update Files for next run
          cp current_subdomains.txt prev_output.${{ env.TARGET_DOMAIN }}.txt
          cp $CURR_HTTPX $PREV_HTTPX

      - name: Commit and Push Changes
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config --global user.name 'github-actions'
          git config --global user.email 'github-actions@github.com'
          git add prev_output.*.txt prev_httpx_res.txt
          
          if git diff --staged --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Update scan results for ${{ env.TARGET_DOMAIN }}"
            git push
          fi
