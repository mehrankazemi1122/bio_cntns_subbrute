name: Continuous Subdomain Brute Force

on:
  push:
    branches: [ main ]
  schedule:
    # Runs every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch:

env:
  # CHANGE THIS TO YOUR TARGET DOMAIN
  TARGET_DOMAIN: "verisign.com" 
  DISCORD_TOKEN: ${{ secrets.DISCORD_TOKEN }}

jobs:
  # ==============================================================================
  # JOB 1: PREPARE
  # Generates wordlists, resolvers, and splits the work into 20 chunks.
  # ==============================================================================
  prepare:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Install MassDNS & Utils
        run: |
          sudo apt-get install -y jq curl wget libpcap-dev crunch
          git clone https://github.com/blechschmidt/massdns.git
          cd massdns && make && sudo cp bin/massdns /usr/local/bin/

      - name: Generate Resolvers
        run: |
          wget https://raw.githubusercontent.com/trickest/resolvers/main/resolvers.txt -O raw_resolvers.txt
          echo 8.8.8.8 > trusted_resolver.txt
          massdns -r trusted_resolver.txt -t A -o J -w verified_resolvers.json --verify-ip raw_resolvers.txt -s 5000
          cat verified_resolvers.json | jq -r '.["name"]' | sed 's/\.$//' > resolvers.txt
          head resolvers.txt
          tail resolvers.txt

      - name: Generate & Merge Wordlists
        run: |
          crunch 1 4 abcdefghijklmnopqrstuvwxyz1234567890 > 4_chars.txt
          
          # Download wordlists
          curl -s https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/DNS/combined_subdomains.txt > combined.txt
          curl -s https://raw.githubusercontent.com/n0kovo/n0kovo_subdomains/main/n0kovo_subdomains_huge.txt > kovo.txt
          curl -s https://wordlists-cdn.assetnote.io/data/manual/best-dns-wordlist.txt > best-dns.txt
          curl -s https://wordlists-cdn.assetnote.io/data/manual/2m-subdomains.txt > 2m.txt
          curl -s https://wordlists-cdn.assetnote.io/data/automated/httparchive_subdomains_2024_01_28.txt > all_subs.txt
          curl -s https://media.githubusercontent.com/media/c3l3si4n/subdomain_data/refs/heads/main/common_nodes_6_occurrences.txt > 7mil.txt
          curl -s https://raw.githubusercontent.com/mehrankazemi1122/bio_cntns_subbrute/refs/heads/main/spec_subs.txt > spec_subs.txt          
          # Merge, Sort, Unique
          cat best-dns.txt 2m.txt 4_chars.txt all_subs.txt combined.txt kovo.txt 7mil.txt spec_subs.txt | sort -u > merged_wordlist.txt
          echo "Total words: $(wc -l < merged_wordlist.txt)"

      - name: Split Wordlist
        run: |
          mkdir chunks
          # Split into 20 numeric suffixes (00 to 19)
          split -d -n l/20 merged_wordlist.txt chunks/wordlist_part_

      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: preparation-data
          path: |
            resolvers.txt
            chunks/*

  # ==============================================================================
  # JOB 2: MATRIX SCAN
  # Runs Shuffledns -> Dnsx -> Httpx inside the matrix
  # ==============================================================================
  scan:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        part: [00, 01, 02, 03, 04, 05, 06, 07, 08, 09, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
    steps:
      - name: Download Preparation Data
        uses: actions/download-artifact@v4
        with:
          name: preparation-data

      - name: Install Tools
        run: |
          # MassDNS
          sudo apt-get install -y libpcap-dev
          git clone https://github.com/blechschmidt/massdns.git
          cd massdns && make && sudo cp bin/massdns /usr/local/bin/
          cd ..
          
          # Go Tools
          go install -v github.com/projectdiscovery/shuffledns/cmd/shuffledns@latest
          go install -v github.com/projectdiscovery/dnsx/cmd/dnsx@latest
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest

      - name: Run Pipeline on Chunk ${{ matrix.part }}
        run: |
          WORDLIST="chunks/wordlist_part_${{ matrix.part }}"
          
          # 1. Bruteforce Subdomains (shuffledns)
          echo "Running shuffledns..."
          shuffledns -w $WORDLIST -d ${{ env.TARGET_DOMAIN }} -r resolvers.txt -mode bruteforce -retries 3 -o raw_subs_${{ matrix.part }}.txt
          
          # 2. Validate Subdomains (dnsx)
          # We filter strictly for the target domain to clean up wildcards/errors
          echo "Running dnsx validation..."
          cat raw_subs_${{ matrix.part }}.txt | grep "${{ env.TARGET_DOMAIN }}$" | dnsx -silent -r resolvers.txt -o valid_subs_${{ matrix.part }}.txt
          
          # 3. HTTP Probing (httpx)
          # Runs ONLY on the valid subdomains from step 2
          echo "Running httpx..."
          httpx -l valid_subs_${{ matrix.part }}.txt \
            -follow-host-redirects -nc -title -status-code -cdn -tech-detect \
            -H "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:108.0) Gecko/20100101 Firefox/108.0" \
            -silent -o httpx_res_${{ matrix.part }}.txt

      - name: Upload Chunk Results
        uses: actions/upload-artifact@v4
        with:
          name: scan-results-${{ matrix.part }}
          path: |
            valid_subs_${{ matrix.part }}.txt
            httpx_res_${{ matrix.part }}.txt

  # ==============================================================================
  # JOB 3: FINALIZE
  # Combines results, compares with previous run, and notifies Discord
  # ==============================================================================
  finalize:
    needs: scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2

      - name: Download All Scan Results
        uses: actions/download-artifact@v4
        with:
          pattern: scan-results-*
          merge-multiple: true

      - name: Merge Results
        run: |
          # Combine Valid Subdomains (for record keeping)
          cat valid_subs_*.txt | sort -u > current_subdomains.txt
          
          # Combine HTTPX Results
          cat httpx_res_*.txt | sort -u > current_httpx.txt

      - name: Compare & Notify (HTTPX Only)
        run: |
          PREV_HTTPX="prev_httpx_res.txt"
          CURR_HTTPX="current_httpx.txt"
          touch $PREV_HTTPX
          
          # 1. Detect New Services (URL is new)
          awk '{print $1}' $PREV_HTTPX | sort > prev_urls.txt
          awk '{print $1}' $CURR_HTTPX | sort > curr_urls.txt
          
          comm -13 prev_urls.txt curr_urls.txt > new_urls_keys.txt
          
          if [ -s new_urls_keys.txt ]; then
            echo "New services found..."
            # grep -Ff reads patterns from file (fixed strings)
            grep -Ff new_urls_keys.txt $CURR_HTTPX > added_services.txt
            
            # Send Discord Notification
            MSG="**Httpx - Services Added:**\n\`\`\`\n$(cat added_services.txt | head -c 1900)\n\`\`\`"
            JSON_PAYLOAD=$(jq -n --arg content "$MSG" '{content: $content}')
            curl -H "Content-Type: application/json" -d "$JSON_PAYLOAD" "${{ env.DISCORD_TOKEN }}"
          fi
          
          # 2. Detect Status Changes (URL exists, Status Changed)
          python3 -c "
          import sys
          
          def load_map(fname):
              d = {}
              try:
                  with open(fname, 'r') as f:
                      for line in f:
                          parts = line.strip().split()
                          if len(parts) > 1:
                              # Key: URL, Value: [StatusCode]
                              d[parts[0]] = parts[1]
              except FileNotFoundError:
                  pass
              return d

          prev = load_map('$PREV_HTTPX')
          curr_lines = open('$CURR_HTTPX', 'r').readlines()
          
          changes = []
          for line in curr_lines:
              parts = line.strip().split()
              if len(parts) > 1:
                  url = parts[0]
                  status = parts[1]
                  # If URL was in previous run, but status is different
                  if url in prev and prev[url] != status:
                      changes.append(f'URL: {url} | Old: {prev[url]} -> New: {status}')
          
          if changes:
              print('\n'.join(changes))
          " > changed_services.txt
          
          if [ -s changed_services.txt ]; then
             echo "Status changes found..."
             MSG="**Httpx - Services Changed:**\n\`\`\`\n$(cat changed_services.txt | head -c 1900)\n\`\`\`"
             JSON_PAYLOAD=$(jq -n --arg content "$MSG" '{content: $content}')
             curl -H "Content-Type: application/json" -d "$JSON_PAYLOAD" "${{ env.DISCORD_TOKEN }}"
          fi
          
          # Update Files for next run
          cp current_subdomains.txt prev_output.${{ env.TARGET_DOMAIN }}.txt
          cp $CURR_HTTPX $PREV_HTTPX

      - name: Commit and Push Changes
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config --global user.name 'github-actions'
          git config --global user.email 'github-actions@github.com'
          
          git add prev_output.*.txt prev_httpx_res.txt
          
          if git diff --staged --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Update scan results for ${{ env.TARGET_DOMAIN }}"
            git push
          fi
